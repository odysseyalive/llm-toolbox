{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HuggingFace Model to GGUF Format\n",
    "\n",
    "This notebook demonstrates how to mount your drive for persistence, check if the HuggingFace model has already been downloaded, and then convert the model to the GGUF file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Drive for Persistence\n",
    "\n",
    "The code below mounts your drive when running in Google Colab, ensuring that files are persisted.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs('/content/drive/My Drive/llm', exist_ok=True)\n",
    "os.chdir('/content/drive/My Drive/llm')\n",
    "print('Current directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install requirements\n",
    "\n",
    "This cell checks if the model has already been downloaded to avoid redundant downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install numpy huggingface_hub safetensors torch llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download HuggingFace Model if Not Already Present\n",
    "\n",
    "This cell checks if the model has already been downloaded to avoid redundant downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
   "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_repo = \"tomg-group-umd/huginn-0125\"\n",
    "cache_dir = os.getcwd()\n",
    "expected_model_dir = os.path.join(cache_dir, model_repo.replace('/', '-'))\n",
    "\n",
    "print(\"\\nStarting model download...\")\n",
    "if os.path.exists(expected_model_dir):\n",
    "    print(f\"Found cached model at: {expected_model_dir}\")\n",
    "    model_path = expected_model_dir\n",
    "else:\n",
    "    print(f\"Downloading {model_repo} from Hugging Face Hub...\")\n",
    "    model_path = snapshot_download(repo_id=model_repo, cache_dir=cache_dir)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "print('\\nModel path:', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert the Model to GGUF Format\n",
    "\n",
    "This cell uses your hypothetical `gguf` library to convert the downloaded HuggingFace model to the GGUF file format. It also checks whether the conversion has been performed already to avoid redundant processing.\n",
    "\n",
    "Note: Adjust the conversion code based on the actual GGUF conversion tool/library you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from gguf import convert\n",
    "import os\n",
    "\n",
    "# Define the output file path for the GGUF model\n",
    "output_path = os.path.join(os.getcwd(), f\"output_model.gguf\")\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    print(f\"Converting model to GGUF format and saving to '{output_path}'...\")\n",
    "    convert.save_model(model, output_path, quantization='q8_0')\n",
    "    print(f\"Model converted and saved as '{output_path}'.\")\n",
    "else:\n",
    "    print(f\"GGUF model file '{output_path}' already exists. Skipping conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify",
   "metadata": {},
   "source": [
    "## Step 5: Verify the Conversion\n",
    "\n",
    "Optionally, you can verify the converted GGUF model by attempting to load it. This assumes that the `gguf` library offers a corresponding loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load function from the gguf package\n",
    "from gguf import load_model\n",
    "\n",
    "# Load the converted GGUF model\n",
    "gguf_model = load_model(output_path)\n",
    "\n",
    "print(\"GGUF model loaded successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
