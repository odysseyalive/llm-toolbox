{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desc-mount-drive",
   "metadata": {},
   "source": [
    "## Mounting Your Drive\n",
    "\n",
    "Mount your Google Drive so that the downloaded model and output GGUF file are stored persistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mount-drive-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create a directory for the LLM models if it doesn't exist\n",
    "os.makedirs('/content/drive/My Drive/llm', exist_ok=True)\n",
    "\n",
    "# Change working directory to that folder\n",
    "os.chdir('/content/drive/My Drive/llm')\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-intro",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook converts the Huginn‑0125 model to a GGUF file for Ollama using a two-step q8 conversion process. It loads the model’s F32 weights, casts them to F16, and then quantizes them to int8 using a computed scale factor. It also writes a header containing metadata (such as gguf_version, model_type, n_vocab, n_embd, and n_layer) that Ollama expects. (Note: Make sure the Huginn‑0125 repository includes its configuration file so that the correct hyperparameters—like n_layer=32—are used.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-install-deps",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "We install NumPy, the Hugging Face Hub, and safetensors to handle the model weights and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy huggingface_hub safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-download-model",
   "metadata": {},
   "source": [
    "## Downloading the Model\n",
    "\n",
    "We download the Huginn‑0125 model from its HF repository. The model is cached in the current directory. If the repository includes a config file (config.json or params.json), the conversion will use its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Set the HF repository ID for Huginn-0125\n",
    "model_repo = \"tomg-group-umd/huginn-0125\"\n",
    "cache_dir = os.getcwd()\n",
    "\n",
    "# Construct an expected folder name (replace '/' with '-')\n",
    "expected_model_dir = os.path.join(cache_dir, model_repo.replace('/', '-'))\n",
    "\n",
    "if os.path.exists(expected_model_dir):\n",
    "    print(f\"Model already downloaded at: {expected_model_dir}\")\n",
    "    model_path = expected_model_dir\n",
    "else:\n",
    "    print(f\"Downloading model {model_repo}...\")\n",
    "    model_path = snapshot_download(repo_id=model_repo, cache_dir=cache_dir)\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "print('Final model path:', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-load-convert",
   "metadata": {},
   "source": [
    "## Loading and Converting the Model\n",
    "\n",
    "We load the model’s tensors from safetensors and its hyperparameters from the configuration file. Then, for q8 quantization, each tensor is first cast to F16 and then quantized to int8 using a computed scale factor. Finally, we write a GGUF file that begins with a header containing all required metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-convert-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from safetensors import safe_open\n",
    "\n",
    "########################################\n",
    "# Functions to load model and hyperparameters\n",
    "########################################\n",
    "\n",
    "def load_hf_model(model_dir):\n",
    "    \"\"\"\n",
    "    Recursively scan the model directory for .safetensors files and load all tensors.\n",
    "    Returns a dictionary mapping tensor names to NumPy arrays.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".safetensors\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Loading tensors from {file_path}\")\n",
    "                with safe_open(file_path, framework=\"np\") as f:\n",
    "                    for key in f.keys():\n",
    "                        if key in model:\n",
    "                            print(f\"Warning: key {key} already exists. Overwriting.\")\n",
    "                        model[key] = f.get_tensor(key)\n",
    "    return model\n",
    "\n",
    "def load_hf_hparams(model_dir):\n",
    "    \"\"\"\n",
    "    Load hyperparameters from a config file in the model directory.\n",
    "    Tries config.json first, then params.json.\n",
    "    \"\"\"\n",
    "    for fname in [\"config.json\", \"params.json\"]:\n",
    "        config_path = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(config_path):\n",
    "            print(f\"Loading hyperparameters from {config_path}\")\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "    raise ValueError(\"No config.json or params.json found in the model directory.\")\n",
    "\n",
    "########################################\n",
    "# GGUF Writer with header metadata\n",
    "########################################\n",
    "\n",
    "class GGUFWriter:\n",
    "    def __init__(self, outfile, hparams, outtype):\n",
    "        self.outfile = outfile\n",
    "        self.hparams = hparams\n",
    "        self.outtype = outtype  # e.g., \"q8\", \"f16\", or \"f32\"\n",
    "        self.tensors = []\n",
    "\n",
    "    def add_tensor(self, name, tensor, scale=None):\n",
    "        self.tensors.append((name, tensor, scale))\n",
    "        if scale is not None:\n",
    "            print(f\"Added tensor: {name}, shape: {tensor.shape}, dtype: {tensor.dtype}, scale: {scale}\")\n",
    "        else:\n",
    "            print(f\"Added tensor: {name}, shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
    "\n",
    "    def finalize(self):\n",
    "        with open(self.outfile, \"wb\") as f:\n",
    "            # Write header with required metadata\n",
    "            header = \"gguf_version: 1\\n\"\n",
    "            header += f\"model_type: {self.hparams.get('model_type', 'llama')}\\n\"\n",
    "            header += f\"n_vocab: {self.hparams.get('vocab_size', 32000)}\\n\"\n",
    "            header += f\"n_embd: {self.hparams.get('n_embd', 4096)}\\n\"\n",
    "            header += f\"n_layer: {self.hparams.get('n_layer', 32)}\\n\"\n",
    "            header += f\"outtype: {self.outtype}\\n\"\n",
    "            f.write(header.encode('utf-8'))\n",
    "            f.write(b\"--TENSORS--\\n\")\n",
    "\n",
    "            # Write each tensor's metadata and raw data\n",
    "            for name, tensor, scale in self.tensors:\n",
    "                meta = f\"{name} | shape: {tensor.shape} | dtype: {tensor.dtype}\" \n",
    "                if scale is not None:\n",
    "                    meta += f\" | scale: {scale}\" \n",
    "                meta += \"\\n\"\n",
    "                f.write(meta.encode('utf-8'))\n",
    "                f.write(tensor.tobytes())\n",
    "                f.write(b\"\\n\")\n",
    "\n",
    "        print(f\"Finalized GGUF file at {self.outfile}\")\n",
    "\n",
    "########################################\n",
    "# Quantization from F16 to q8\n",
    "########################################\n",
    "\n",
    "def quantize_from_f16(tensor_f16):\n",
    "    # Compute scale factor: maximum absolute value divided by 127\n",
    "    scale = np.max(np.abs(tensor_f16)) / 127.0\n",
    "    if scale == 0:\n",
    "        scale = 1.0\n",
    "    quantized = np.round(tensor_f16 / scale).astype(np.int8)\n",
    "    return quantized, scale\n",
    "\n",
    "########################################\n",
    "# Conversion function for different output types\n",
    "########################################\n",
    "\n",
    "def convert_tensor(tensor, outtype):\n",
    "    if outtype == \"q8\":\n",
    "        # First cast to F16, then quantize from F16 to q8\n",
    "        tensor_f16 = tensor.astype(np.float16)\n",
    "        quantized, scale = quantize_from_f16(tensor_f16)\n",
    "        return quantized, scale\n",
    "    elif outtype == \"f16\":\n",
    "        return tensor.astype(np.float16), None\n",
    "    elif outtype == \"f32\":\n",
    "        return tensor.astype(np.float32), None\n",
    "    else:\n",
    "        return tensor, None\n",
    "\n",
    "def convert_model_to_gguf(model, hparams, outfile, outtype):\n",
    "    writer = GGUFWriter(outfile, hparams, outtype)\n",
    "    for name, tensor in model.items():\n",
    "        converted, scale = convert_tensor(tensor, outtype)\n",
    "        writer.add_tensor(name, converted, scale=scale)\n",
    "    writer.finalize()\n",
    "    print(f\"GGUF conversion complete: {outfile}\")\n",
    "\n",
    "########################################\n",
    "# End conversion functions\n",
    "########################################\n",
    "\n",
    "# Load the HF model from the downloaded directory\n",
    "print(\"Loading HF model from:\", model_path)\n",
    "real_model = load_hf_model(model_path)\n",
    "print(f\"Loaded {len(real_model)} tensors from the model.\")\n",
    "\n",
    "# Load hyperparameters from config.json or params.json\n",
    "real_hparams = load_hf_hparams(model_path)\n",
    "print(\"Hyperparameters loaded.\")\n",
    "\n",
    "# Define output GGUF file and conversion type\n",
    "output_filename = \"output_model.gguf\"\n",
    "output_type = \"q8\"\n",
    "\n",
    "# Convert the model to GGUF using two-step quantization from F16\n",
    "convert_model_to_gguf(real_model, real_hparams, output_filename, outtype=output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-list-dir",
   "metadata": {},
   "source": [
    "## Verifying the Output\n",
    "\n",
    "Finally, list the contents of the working directory to verify that the GGUF file was created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-dir-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Real_HF_Model_to_GGUF_q8_from_f16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
