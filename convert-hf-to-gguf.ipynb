{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desc-mount-drive",
   "metadata": {},
   "source": [
    "## Mounting Your Drive\n",
    "\n",
    "Mount your Google Drive to store files persistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mount-drive-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs('/content/drive/My Drive/llm', exist_ok=True)\n",
    "os.chdir('/content/drive/My Drive/llm')\n",
    "print('Current directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-intro",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Convert Hugging Face models to GGUF format with Q8_0 quantization.\n",
    "\n",
    "This version now constructs a proper Q8_0 byte layout. For each 32‚Äêelement block of tensor data, we compute a scale (a float32) and store 32 quantized int8s. The GGUF writer is updated accordingly to accept raw byte buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy huggingface_hub safetensors torch llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_repo = \"tomg-group-umd/huginn-0125\"\n",
    "cache_dir = os.getcwd()\n",
    "expected_model_dir = os.path.join(cache_dir, model_repo.replace('/', '-'))\n",
    "\n",
    "print(\"\\nStarting model download...\")\n",
    "if os.path.exists(expected_model_dir):\n",
    "    print(f\"Found cached model at: {expected_model_dir}\")\n",
    "    model_path = expected_model_dir\n",
    "else:\n",
    "    print(f\"Downloading {model_repo} from Hugging Face Hub...\")\n",
    "    model_path = snapshot_download(repo_id=model_repo, cache_dir=cache_dir)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "print('\\nModel path:', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-convert-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_hf_model(model_dir):\n",
    "    print(\"\\nLoading model tensors...\")\n",
    "    model = {}\n",
    "    \n",
    "    # First look for safetensors files\n",
    "    files = [f for f in os.listdir(model_dir) if f.endswith(\".safetensors\")]\n",
    "    \n",
    "    # If no safetensors found, try for .bin files\n",
    "    if not files:\n",
    "        print(\"No .safetensors files found, checking for .bin files...\")\n",
    "        files = [f for f in os.listdir(model_dir) if f.endswith(\".bin\")]\n",
    "        if not files:\n",
    "            raise ValueError(\"No model files found with extensions .safetensors or .bin\")\n",
    "        \n",
    "        # Load .bin files using torch\n",
    "        import torch\n",
    "        for file in files:\n",
    "            file_path = os.path.join(model_dir, file)\n",
    "            print(f\"Loading {file_path} with torch.load...\")\n",
    "            state_dict = torch.load(file_path, map_location=\"cpu\")\n",
    "            for key, tensor in state_dict.items():\n",
    "                if hasattr(tensor, \"detach\"):\n",
    "                    tensor = tensor.detach().cpu().numpy()\n",
    "                model[key] = tensor\n",
    "    else:\n",
    "        # Load using safetensors\n",
    "        from safetensors import safe_open\n",
    "        for file in tqdm(files, desc=\"Processing safetensors files\"):\n",
    "            file_path = os.path.join(model_dir, file)\n",
    "            with safe_open(file_path, framework=\"np\") as f:\n",
    "                for key in f.keys():\n",
    "                    model[key] = f.get_tensor(key)\n",
    "    \n",
    "    print(f\"Loaded {len(model)} tensors\")\n",
    "    return model\n",
    "\n",
    "def load_hf_hparams(model_dir):\n",
    "    print(\"\\nLoading hyperparameters...\")\n",
    "    for fname in [\"config.json\", \"params.json\"]:\n",
    "        config_path = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path) as f:\n",
    "                print(f\"Found config file: {fname}\")\n",
    "                return json.load(f)\n",
    "    raise ValueError(\"No config found\")\n",
    "\n",
    "class GGUFWriter:\n",
    "    def __init__(self, outfile, hparams, outtype):\n",
    "        self.outfile = outfile\n",
    "        self.hparams = hparams\n",
    "        self.outtype = outtype\n",
    "        self.tensors = []\n",
    "        self.data_buffer = bytearray()\n",
    "        self.offset = 0\n",
    "        self.alignment = 32\n",
    "        print(f\"\\nInitialized GGUF writer for {outtype} quantization\")\n",
    "\n",
    "    def add_tensor(self, name, tensor, original_shape, raw_bytes=False):\n",
    "        # If raw_bytes is True, the tensor is already a bytes-like object\n",
    "        if raw_bytes:\n",
    "            tensor_bytes = tensor\n",
    "        else:\n",
    "            tensor_bytes = tensor.tobytes()\n",
    "        pad = (-len(tensor_bytes)) % self.alignment\n",
    "        self.data_buffer += tensor_bytes + bytes(pad)\n",
    "        \n",
    "        self.tensors.append({\n",
    "            \"name\": name.encode(),\n",
    "            \"dtype\": 8 if self.outtype == \"Q8_0\" else 1,\n",
    "            \"shape\": original_shape,\n",
    "            \"offset\": self.offset,\n",
    "            \"size\": len(tensor_bytes) + pad\n",
    "        })\n",
    "        self.offset += len(tensor_bytes) + pad\n",
    "\n",
    "    def finalize(self):\n",
    "        print(\"\\nFinalizing GGUF file...\")\n",
    "        with open(self.outfile, \"wb\") as f:\n",
    "            # Write header\n",
    "            f.write(b\"GGUF\")\n",
    "            f.write(np.uint32(3).tobytes())\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = {\n",
    "                \"general.architecture\": \"llama\",\n",
    "                \"general.name\": self.hparams.get(\"model_type\", \"llama\"),\n",
    "                \"llama.context_length\": self.hparams.get(\"max_position_embeddings\", 2048),\n",
    "                \"llama.embedding_length\": self.hparams.get(\"hidden_size\", 4096),\n",
    "                \"llama.block_count\": self.hparams.get(\"num_hidden_layers\", 32),\n",
    "                \"llama.attention.head_count\": self.hparams.get(\"num_attention_heads\", 32),\n",
    "                \"general.file_type\": 8 if self.outtype == \"Q8_0\" else 1\n",
    "            }\n",
    "            \n",
    "            # Write metadata count\n",
    "            f.write(np.uint64(len(metadata)).tobytes())\n",
    "            print(\"Writing metadata:\")\n",
    "            \n",
    "            for key, val in metadata.items():\n",
    "                # Write key\n",
    "                f.write(np.uint64(len(key)).tobytes())\n",
    "                f.write(key.encode())\n",
    "                \n",
    "                # Handle different value types\n",
    "                if isinstance(val, str):\n",
    "                    # String type (type=1)\n",
    "                    f.write(np.uint32(1).tobytes())\n",
    "                    f.write(np.uint64(len(val)).tobytes())\n",
    "                    f.write(val.encode())\n",
    "                else:\n",
    "                    # UINT32 type (type=4)\n",
    "                    f.write(np.uint32(4).tobytes())\n",
    "                    f.write(np.uint32(val).tobytes())\n",
    "                \n",
    "                print(f\" - {key}: {val}\")\n",
    "            \n",
    "            # Write tensors\n",
    "            print(f\"\\nWriting {len(self.tensors)} tensors...\")\n",
    "            f.write(np.uint64(len(self.tensors)).tobytes())\n",
    "            \n",
    "            for tensor in self.tensors:\n",
    "                f.write(np.uint64(len(tensor[\"name\"])).tobytes())\n",
    "                f.write(tensor[\"name\"])\n",
    "                f.write(np.uint32(tensor[\"dtype\"]).tobytes())\n",
    "                f.write(np.uint32(len(tensor[\"shape\"])).tobytes())\n",
    "                for dim in tensor[\"shape\"]:\n",
    "                    f.write(np.uint64(dim).tobytes())\n",
    "                f.write(np.uint64(tensor[\"offset\"]).tobytes())\n",
    "                f.write(np.uint64(tensor[\"size\"]).tobytes())\n",
    "            \n",
    "            # Write tensor data\n",
    "            print(\"Writing tensor data...\")\n",
    "            f.write(self.data_buffer)\n",
    "        print(f\"\\nGGUF file created: {self.outfile}\")\n",
    "\n",
    "def quantize_q8_0(tensor):\n",
    "    \"\"\"\n",
    "    Quantizes the tensor into Q8_0 format by processing 32-element blocks.\n",
    "    For each block, a scale (as float32) is computed and stored, followed by 32 int8 quantized values.\n",
    "    Returns a bytes object containing the quantized data and the original shape.\n",
    "    \"\"\"\n",
    "    original_shape = tensor.shape\n",
    "    tensor_flat = tensor.flatten().astype(np.float32)\n",
    "    num_blocks = (tensor_flat.size + 31) // 32\n",
    "    padded = np.pad(tensor_flat, (0, num_blocks * 32 - tensor_flat.size))\n",
    "    blocks = padded.reshape(-1, 32)\n",
    "    \n",
    "    # Compute scales for each block\n",
    "    scales = np.max(np.abs(blocks), axis=1, keepdims=True) / 127.0\n",
    "    # Avoid division by zero\n",
    "    scales[scales == 0] = 1.0\n",
    "    \n",
    "    # Quantize blocks using the scales\n",
    "    quantized = np.round(blocks / scales).astype(np.int8)\n",
    "    \n",
    "    # Build a bytes buffer for the quantized data\n",
    "    qbuffer = bytearray()\n",
    "    for scale, q in zip(scales, quantized):\n",
    "        # Write the scale as a 4-byte float32\n",
    "        qbuffer.extend(np.array(scale, dtype=np.float32).tobytes())\n",
    "        # Write the 32 quantized int8 values\n",
    "        qbuffer.extend(q.tobytes())\n",
    "    \n",
    "    return qbuffer, original_shape\n",
    "\n",
    "def convert_model_to_gguf(model, hparams, outfile, outtype):\n",
    "    print(\"\\nStarting conversion to GGUF...\")\n",
    "    writer = GGUFWriter(outfile, hparams, outtype)\n",
    "    total_tensors = len(model)\n",
    "    \n",
    "    for i, (name, tensor) in enumerate(tqdm(model.items(), desc=\"Processing tensors\")):\n",
    "        if outtype == \"Q8_0\":\n",
    "            quantized_data, original_shape = quantize_q8_0(tensor)\n",
    "            # Do not cast further; pass the raw bytes and indicate raw_bytes=True\n",
    "            writer.add_tensor(name, quantized_data, original_shape, raw_bytes=True)\n",
    "        else:\n",
    "            writer.add_tensor(name, tensor.astype(np.float32), tensor.shape)\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{total_tensors} tensors\")\n",
    "    \n",
    "    writer.finalize()\n",
    "\n",
    "# Execute conversion\n",
    "print(\"\\n=== Conversion Process Starting ===\")\n",
    "real_model = load_hf_model(model_path)\n",
    "real_hparams = load_hf_hparams(model_path)\n",
    "convert_model_to_gguf(real_model, real_hparams, \"output_model.gguf\", \"Q8_0\")\n",
    "print(\"\\n=== Conversion Complete ===\")\n",
    "\n",
    "# Validation\n",
    "print(\"\\nValidating GGUF file...\")\n",
    "if not os.path.exists(\"output_model.gguf\"):\n",
    "    print(\"Validation failed: output_model.gguf does not exist.\")\n",
    "else:\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        print(\"Attempting to load the GGUF file using Llama...\")\n",
    "        llm = Llama(model_path=\"output_model.gguf\")\n",
    "        print(\"Validation successful! GGUF file loaded correctly.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {str(e)}\")\n",
    "\n",
    "!ls -lh *.gguf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
