{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desc-mount-drive",
   "metadata": {},
   "source": [
    "## Mounting Your Drive\n",
    "\n",
    "In this first step, we mount your Google Drive and set the working directory. This ensures that all files are stored persistently and that we have a defined workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mount-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create a directory for the LLM models if it doesn't exist\n",
    "os.makedirs('/content/drive/My Drive/llm', exist_ok=True)\n",
    "\n",
    "# Change working directory to the new folder in Google Drive\n",
    "os.chdir('/content/drive/My Drive/llm')\n",
    "\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-intro",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook converts a Hugging Face model into the GGUF format with q8 quantization. The process involves downloading the model, loading its weights and configuration, and applying quantization to reduce precision to 8-bit. The final output is a GGUF file containing the quantized model data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-install-deps",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "We install the necessary Python libraries: NumPy for numerical operations, Hugging Face Hub for downloading the model, and safetensors for reading model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "!pip install numpy huggingface_hub safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-download-model",
   "metadata": {},
   "source": [
    "## Downloading the Model\n",
    "\n",
    "Using the Hugging Face Hub, we download the model specified by its repository ID. The model is stored in the current working directory. Adjust the repository ID (model_repo) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Set the HF repository ID for your model\n",
    "model_repo = \"tomg-group-umd/huginn-0125\"\n",
    "cache_dir = os.getcwd()\n",
    "\n",
    "# Construct an expected folder name based on the repo id (replace '/' with '-')\n",
    "expected_model_dir = os.path.join(cache_dir, model_repo.replace('/', '-'))\n",
    "\n",
    "if os.path.exists(expected_model_dir):\n",
    "    print(f\"Model already downloaded at: {expected_model_dir}\")\n",
    "    model_path = expected_model_dir\n",
    "else:\n",
    "    print(f\"Downloading model {model_repo}...\")\n",
    "    model_path = snapshot_download(repo_id=model_repo, cache_dir=cache_dir)\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "print('Final model path:', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-load-convert",
   "metadata": {},
   "source": [
    "## Loading and Converting the Model\n",
    "\n",
    "We load the model's tensors from the safetensors files and its hyperparameters from a configuration file (config.json or params.json). The conversion process then applies q8 quantization to each tensor and writes the output in a simplified GGUF format. The quantization reduces precision to 8-bit and stores a scale factor for each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-and-convert-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from safetensors import safe_open\n",
    "\n",
    "########################################\n",
    "# HF Model loading and conversion code with q8 quantization\n",
    "########################################\n",
    "\n",
    "def load_hf_model(model_dir):\n",
    "    \"\"\"\n",
    "    Recursively scan the model directory for .safetensors files and load all tensors.\n",
    "    Returns a dictionary mapping tensor names to NumPy arrays.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".safetensors\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Loading tensors from {file_path}\")\n",
    "                with safe_open(file_path, framework=\"np\") as f:\n",
    "                    for key in f.keys():\n",
    "                        if key in model:\n",
    "                            print(f\"Warning: key {key} already exists. Overwriting.\")\n",
    "                        model[key] = f.get_tensor(key)\n",
    "    return model\n",
    "\n",
    "def load_hf_hparams(model_dir):\n",
    "    \"\"\"\n",
    "    Load hyperparameters from a config file in the model directory.\n",
    "    Tries config.json first, then params.json.\n",
    "    \"\"\"\n",
    "    for fname in [\"config.json\", \"params.json\"]:\n",
    "        config_path = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(config_path):\n",
    "            print(f\"Loading hyperparameters from {config_path}\")\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "    raise ValueError(\"No config.json or params.json found in the model directory.\")\n",
    "\n",
    "########################################\n",
    "# Minimal GGUF writer (simplified example)\n",
    "########################################\n",
    "\n",
    "class GGUFWriter:\n",
    "    def __init__(self, outfile, hparams, outtype):\n",
    "        self.outfile = outfile\n",
    "        self.hparams = hparams  # hyperparameters dictionary\n",
    "        self.outtype = outtype  # e.g., \"q8\", \"f16\", or \"f32\"\n",
    "        self.tensors = []\n",
    "\n",
    "    def add_tensor(self, name, tensor, scale=None):\n",
    "        self.tensors.append((name, tensor, scale))\n",
    "        if scale is not None:\n",
    "            print(f\"Added tensor: {name}, shape: {tensor.shape}, dtype: {tensor.dtype}, scale: {scale}\")\n",
    "        else:\n",
    "            print(f\"Added tensor: {name}, shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
    "\n",
    "    def finalize(self):\n",
    "        with open(self.outfile, \"wb\") as f:\n",
    "            # Write a simple header (placeholder for the GGUF header)\n",
    "            f.write(b\"GGUF\\n\")\n",
    "\n",
    "            # Write hyperparameters as text\n",
    "            for key, value in self.hparams.items():\n",
    "                line = f\"{key}: {value}\\n\".encode('utf-8')\n",
    "                f.write(line)\n",
    "\n",
    "            f.write(b\"--TENSORS--\\n\")\n",
    "\n",
    "            # Write each tensor's metadata and raw data\n",
    "            for name, tensor, scale in self.tensors:\n",
    "                meta = f\"{name} | shape: {tensor.shape} | dtype: {tensor.dtype}\".encode('utf-8')\n",
    "                if scale is not None:\n",
    "                    meta += f\" | scale: {scale}\".encode('utf-8')\n",
    "                meta += b\"\\n\"\n",
    "                f.write(meta)\n",
    "                f.write(tensor.tobytes())\n",
    "                f.write(b\"\\n\")\n",
    "\n",
    "        print(f\"Finalized GGUF file at {self.outfile}\")\n",
    "\n",
    "########################################\n",
    "# Quantization function for q8 output\n",
    "########################################\n",
    "\n",
    "def quantize_to_q8(tensor):\n",
    "    # Ensure tensor is float32\n",
    "    tensor_f32 = tensor.astype(np.float32)\n",
    "    # Compute scale factor: max absolute value divided by 127\n",
    "    scale = np.max(np.abs(tensor_f32)) / 127.0\n",
    "    if scale == 0:\n",
    "        scale = 1.0\n",
    "    # Quantize: divide by scale, round, then cast to int8\n",
    "    quantized = np.round(tensor_f32 / scale).astype(np.int8)\n",
    "    return quantized, scale\n",
    "\n",
    "########################################\n",
    "# Conversion function that handles different output types\n",
    "########################################\n",
    "\n",
    "def convert_tensor(tensor, outtype):\n",
    "    if outtype == \"q8\":\n",
    "        quantized, scale = quantize_to_q8(tensor)\n",
    "        return quantized, scale\n",
    "    elif outtype == \"f16\":\n",
    "        return tensor.astype(np.float16), None\n",
    "    elif outtype == \"f32\":\n",
    "        return tensor.astype(np.float32), None\n",
    "    else:\n",
    "        return tensor, None\n",
    "\n",
    "def convert_model_to_gguf(model, hparams, outfile, outtype=\"q8\"):\n",
    "    writer = GGUFWriter(outfile, hparams, outtype)\n",
    "    for tensor_name, tensor_data in model.items():\n",
    "        converted, scale = convert_tensor(tensor_data, outtype)\n",
    "        writer.add_tensor(tensor_name, converted, scale=scale)\n",
    "    writer.finalize()\n",
    "    print(f\"GGUF conversion complete: {outfile}\")\n",
    "\n",
    "########################################\n",
    "# End of conversion code\n",
    "########################################\n",
    "\n",
    "# Load your HF model from the downloaded directory\n",
    "print(\"Loading HF model from:\", model_path)\n",
    "real_model = load_hf_model(model_path)\n",
    "print(f\"Loaded {len(real_model)} tensors from the model.\")\n",
    "\n",
    "# Load hyperparameters from config.json (or params.json)\n",
    "real_hparams = load_hf_hparams(model_path)\n",
    "print(\"Hyperparameters loaded.\")\n",
    "\n",
    "# Define the output GGUF file\n",
    "output_filename = \"output_model.gguf\"\n",
    "output_type = \"q8\"\n",
    "\n",
    "# Convert the real model to GGUF with q8 quantization\n",
    "convert_model_to_gguf(real_model, real_hparams, output_filename, outtype=output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-list-dir",
   "metadata": {},
   "source": [
    "## Verifying the Output\n",
    "\n",
    "Finally, we list the contents of our working directory to verify that the GGUF file has been created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-dir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the current directory to verify the output file\n",
    "!ls -lh"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Real_HF_Model_to_GGUF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
